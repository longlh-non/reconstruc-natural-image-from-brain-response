{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lhlong0550/reconstruct-natural-image-from-brain-activity?scriptVersionId=158860178\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"**KAY dataset: Exploration**\n\nIn this colab,we are going to explore and understand the data. I encourage you to do your own experiments","metadata":{"id":"NOSIm6-KF87Q"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport urllib\nimport random\nimport pandas as pd # Library for data analysis and visualization","metadata":{"id":"EFxeH9vrFPnV","execution":{"iopub.status.busy":"2024-01-04T14:21:02.627281Z","iopub.status.idle":"2024-01-04T14:21:02.627739Z","shell.execute_reply.started":"2024-01-04T14:21:02.627501Z","shell.execute_reply":"2024-01-04T14:21:02.627523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First download the data:","metadata":{"id":"LC9XwoOjGSnY"}},{"cell_type":"code","source":"fname = \"kay_labels.npy\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/r638s/download\nfname = \"kay_labels_val.npy\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/yqb3e/download\nfname = \"kay_images.npz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/ymnjv/download","metadata":{"id":"Q6yxQ_U-GH-m","execution":{"iopub.status.busy":"2024-01-04T14:21:02.628833Z","iopub.status.idle":"2024-01-04T14:21:02.629325Z","shell.execute_reply.started":"2024-01-04T14:21:02.629068Z","shell.execute_reply":"2024-01-04T14:21:02.62909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then load the data:","metadata":{"id":"6Ur6rvu-GXfy"}},{"cell_type":"code","source":"with np.load(fname) as dobj:\n    dat = dict(**dobj)\n\nlabels = np.load('kay_labels.npy')\nval_labels = np.load('kay_labels_val.npy')","metadata":{"id":"OD6Dg94wGVIg","execution":{"iopub.status.busy":"2024-01-04T14:21:02.630659Z","iopub.status.idle":"2024-01-04T14:21:02.631011Z","shell.execute_reply.started":"2024-01-04T14:21:02.630833Z","shell.execute_reply":"2024-01-04T14:21:02.630849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exploring the labels for the training and validation sets. There are 4 rows, the fourth row has the label (as predicted by a DNN on imagenet). Rows 1-3 have contain 3 different levels of the wordnet hierarchy.\n\n**Exercise:** Learn what imagenet and wordnet are.","metadata":{"id":"2FwtUBoxdOhp"}},{"cell_type":"code","source":"print(labels.shape) # 4 rows\n\nprint(\"Number of image: \", labels.shape[1])\nprint(\"Number of label for each image: \", labels.shape[0])\n\nprint('Example:')\nk = 5\nprint(f'label: {labels[3, k]} which is a/an -> {labels[0, k]} that is a/an -> {labels[1, k]} as a/an -> {labels[2, k]}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7PCWVXg9dbWF","outputId":"a983a6be-f22a-490d-e503-6fd6ad0fe841","execution":{"iopub.status.busy":"2024-01-04T14:21:02.633803Z","iopub.status.idle":"2024-01-04T14:21:02.634162Z","shell.execute_reply.started":"2024-01-04T14:21:02.633989Z","shell.execute_reply":"2024-01-04T14:21:02.634007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a look at some of the labels:","metadata":{"id":"G0x1JMDHiaw5"}},{"cell_type":"code","source":"print(val_labels[:, 6:10])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kEoxFc2dhy9","outputId":"b61d10e6-60cf-4537-cf50-d86e4108cd62","execution":{"iopub.status.busy":"2024-01-04T14:21:02.635709Z","iopub.status.idle":"2024-01-04T14:21:02.636026Z","shell.execute_reply.started":"2024-01-04T14:21:02.635868Z","shell.execute_reply":"2024-01-04T14:21:02.635883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Learn about the different categories:","metadata":{"id":"NtNregoriixs"}},{"cell_type":"code","source":"print(np.unique(labels[0]))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJ1T1TIpipJ4","outputId":"15c8d237-ca4c-4ea2-eddd-a8694059e369","execution":{"iopub.status.busy":"2024-01-04T14:21:02.637779Z","iopub.status.idle":"2024-01-04T14:21:02.63813Z","shell.execute_reply.started":"2024-01-04T14:21:02.63796Z","shell.execute_reply":"2024-01-04T14:21:02.637977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How many examples for each category:","metadata":{"id":"ysg0d0SLiz55"}},{"cell_type":"code","source":"for cat in np.unique(labels[0]):\n  print(cat, (labels.T == cat).sum())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fp2o6vdwi_xJ","outputId":"b4af8628-cdd8-404e-af60-a579a7f243da","execution":{"iopub.status.busy":"2024-01-04T14:21:02.639313Z","iopub.status.idle":"2024-01-04T14:21:02.639651Z","shell.execute_reply.started":"2024-01-04T14:21:02.639491Z","shell.execute_reply":"2024-01-04T14:21:02.639506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploring the data:**\n\n`dat` is a dictionary","metadata":{"id":"5C1F_x85jCmN"}},{"cell_type":"code","source":"print(dat.keys())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qERr7MgmjAYU","outputId":"db163b55-ee94-4e37-bc39-51f06d3609bd","execution":{"iopub.status.busy":"2024-01-04T14:21:02.641462Z","iopub.status.idle":"2024-01-04T14:21:02.641807Z","shell.execute_reply.started":"2024-01-04T14:21:02.641642Z","shell.execute_reply":"2024-01-04T14:21:02.641658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`dat` has the following fields:  \n- `stimuli`: stim x i x j array of grayscale stimulus images\n- `stimuli_test`: stim x i x j array of grayscale stimulus images in the test set  \n- `responses`: stim x voxel array of z-scored BOLD response amplitude\n- `responses_test`:  stim x voxel array of z-scored BOLD response amplitude in the test set  \n- `roi`: (region of interest) array of voxel labels\n- `roi_names`: array of names corresponding to voxel labels","metadata":{"id":"1boxmp3DjXD2"}},{"cell_type":"code","source":"print(dat[\"stimuli\"].shape) # 1750 images, each 128x128\nprint(dat[\"stimuli_test\"].shape)\nprint(dat['responses'].shape)\nprint(dat['responses'][18])\nplt.imshow(dat[\"stimuli\"][300], cmap = 'gray')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":538},"id":"XOO1D8YpjVjt","outputId":"6708835a-34dc-4642-9599-f985e8d24a7d","execution":{"iopub.status.busy":"2024-01-04T14:21:02.64343Z","iopub.status.idle":"2024-01-04T14:21:02.643776Z","shell.execute_reply.started":"2024-01-04T14:21:02.643611Z","shell.execute_reply":"2024-01-04T14:21:02.643627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The responses are the \"activation\" of each of the voxels:","metadata":{"id":"4Y0eUNFgkraC"}},{"cell_type":"code","source":"print(dat[\"responses\"].shape) # 8428 voxels in total","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGcc_F-IkWXE","outputId":"191c3304-2718-4fe7-87ea-1455fb032bd9","execution":{"iopub.status.busy":"2024-01-04T14:21:02.64522Z","iopub.status.idle":"2024-01-04T14:21:02.645584Z","shell.execute_reply.started":"2024-01-04T14:21:02.645416Z","shell.execute_reply":"2024-01-04T14:21:02.645433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ROI's are the part of the brain each voxel corresponds to:","metadata":{"id":"VkyWCEXPk-Tp"}},{"cell_type":"code","source":"print(dat['roi'].shape) # each of the voxels has an associated ROI\nprint(dat['responses'][19][275:325])\nprint(dat['roi'][275:325])\nprint(dat['roi_names'][dat['roi'][275:325]]) # ROI has the index of the corresponding region in \"roi_names\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKIpVwBok4gW","outputId":"ee3e910d-6c26-4bd5-dfb5-02335b9b7f8a","execution":{"iopub.status.busy":"2024-01-04T14:21:02.646882Z","iopub.status.idle":"2024-01-04T14:21:02.647201Z","shell.execute_reply.started":"2024-01-04T14:21:02.647045Z","shell.execute_reply":"2024-01-04T14:21:02.64706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.bincount(dat[\"roi\"])) # How many voxels per region\nprint(dat[\"roi_names\"]) # all the regions","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buPDMaDlnPes","outputId":"6ea800cb-fa3b-4a47-d812-66740e7d1dec","execution":{"iopub.status.busy":"2024-01-04T14:21:02.6483Z","iopub.status.idle":"2024-01-04T14:21:02.648614Z","shell.execute_reply.started":"2024-01-04T14:21:02.648456Z","shell.execute_reply":"2024-01-04T14:21:02.648471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v1_responses = dat['responses'][:, dat['roi'] == 1]\nv2_responses = dat['responses'][:, dat['roi'] == 2]\nv3_responses = dat['responses'][:, dat['roi'] == 3]\nv4_responses = dat['responses'][:, dat['roi'] == 6]\n\n# do experiment for each roi\n# train them to get the feature vector -> try to generate the image from the feature vector\n# from feature vector -> calculate the loss","metadata":{"id":"WbSn9_vgAZQV","execution":{"iopub.status.busy":"2024-01-04T14:21:02.64974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at some of the images:","metadata":{"id":"lselIjAHoLAc"}},{"cell_type":"code","source":"f, axs = plt.subplots(2, 4, figsize=(12, 6), sharex=True, sharey=True)\nfor ax, im, lbl in zip(axs.flat, dat[\"stimuli\"], labels[-1,:]):\n  ax.imshow(im, cmap=\"gray\")\n  ax.set_title(lbl)\nf.tight_layout()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614},"id":"7Wo9JNBpoEQn","outputId":"29695331-2746-49c0-a527-d53b9090ad02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each stimulus is associated with a pattern of BOLD response across voxels in visual cortex:\n\n**Exercise:** Read what BOLD is in the context of fMRI. Read the basics of fMRI.","metadata":{"id":"7eOChCwgofLC"}},{"cell_type":"code","source":"signal = dat['responses'][200] # Response for stimulis number 8\n\nplt.plot(signal)\n\nplt.xlabel('Voxel index')\nplt.ylabel('Response amplitude') # It is given by the z-score of the signal (it's a way of normalization)\n\nplt.figure()\nplt.imshow(dat['stimuli'][200], cmap = \"gray\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":883},"id":"g9i9FTEHpiy2","outputId":"6a5349ab-a769-4340-b535-2c4d05925ccf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize all the responses for all the stimuli:","metadata":{"id":"ac0QK4aYtEL-"}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 5))\nax.set(xlabel=\"Voxel\", ylabel=\"Stimulus\")\nheatmap = ax.imshow(dat[\"responses\"], aspect=\"auto\", vmin=-1, vmax=1, cmap=\"bwr\") # Shows the\nf.colorbar(heatmap, shrink=.5, label=\"Response amplitude (Z)\")\nf.tight_layout()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":506},"id":"kZHfI-EBoQsc","outputId":"068fe004-0cb1-4d3b-de4c-df229e24cf76","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res1 = dat[\"responses\"][21]\n\nres2 = dat[\"responses\"][51]\n\nsimilarity = np.dot(res1, res2)\n\nplt.figure()\nplt.imshow(dat[\"stimuli\"][21])\n\n\nplt.figure()\nplt.imshow(dat[\"stimuli\"][51])\nprint(similarity)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":868},"id":"uv2I8X6oAYU_","outputId":"c16fecb0-5d90-46d4-e2ff-e06b190a8e07","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And for the validation set:","metadata":{"id":"pCtyUEuOtT33"}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 2.5))\nax.set(xlabel=\"Voxel\", ylabel=\"Test Stimulus\")\nheatmap = ax.imshow(dat[\"responses_test\"], aspect=\"auto\", vmin=-1, vmax=1, cmap=\"bwr\")\nf.colorbar(heatmap, shrink=.75, label=\"Response amplitude (Z)\")\nf.tight_layout()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"id":"1Eth8OBvtV01","outputId":"a5347f8f-7ea0-4a2c-886f-f72200ce7c6c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exercises:**\nSelect 3 images. Make a bar char with the average responses for each ROI for each of the images","metadata":{"id":"X6PX1j61thhE"}},{"cell_type":"code","source":"print('responses shape: ', dat['responses'][180].shape)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uynS45vPtyuI","outputId":"de5fa299-0b8f-47e2-b1dd-436ec58d551f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature extraction Pre-trained models**\n\nFirst list all available models:","metadata":{"id":"S6YNtewiNSwg"}},{"cell_type":"code","source":"import torchvision.models as tm\nimport torch\nimport os\nimport urllib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom PIL import Image","metadata":{"id":"bZWS_oD-NUgL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @title Code from previous lab\nalexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\nalexnet.eval()\n\n# Loading dataset\nfname = \"kay_labels.npy\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/r638s/download\nfname = \"kay_labels_val.npy\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/yqb3e/download\nfname = \"kay_images.npz\"\nif not os.path.exists(fname):\n  !wget -qO $fname https://osf.io/ymnjv/download\n\nwith np.load(fname) as dobj:\n    dat = dict(**dobj)\n\nlabels = np.load('kay_labels.npy')\nval_labels = np.load('kay_labels_val.npy')\n\n# Example image\nstimulis = dat['stimuli']\nim = dat['stimuli'][18]\n# print(im.shape)\nplt.imshow(im, cmap=\"gray\")\n\n# Testing the classification\npreprocess = transforms.Compose([\n            # transforms.ToPILImage(),\n            transforms.Resize((224,224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\npims = []\nprint(stimulis.shape)\nfor i, sti in enumerate(stimulis):\n  sti = np.stack([im,im,im],axis=2)*255\n\n  pims.append(Image.fromarray(np.uint8(sti))) # To PIL\n\n\n#  np.resize(sti, (224, 224))\nprint(len(pims))\n\ninput_tensor = preprocess(pims[0])\ninput_batch = input_tensor.unsqueeze(0)\nprint(input_tensor.shape)\nwith torch.no_grad():\n    output = alexnet(input_batch)\n\nprobabilities = torch.nn.functional.softmax(output[0], dim=0).detach()\nplt.bar(range(len(probabilities)), probabilities)\n\n# Loading classes\n!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\n# Printing probable classes\nwith open(\"imagenet_classes.txt\", \"r\") as f:\n    categories = [s.strip() for s in f.readlines()]\n\n# Show top categories per image\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(top5_prob.size(0)):\n    print(categories[top5_catid[i]], top5_prob[i].item())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"id":"oOABGIIKGzW1","outputId":"e4dbd7e9-ff20-4547-ae52-0d3fd64b9742","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extracting features from a given layer**","metadata":{"id":"xzzzcpLwRQrv"}},{"cell_type":"code","source":"from torchvision.models.feature_extraction import get_graph_node_names\nnodes, _ = get_graph_node_names(alexnet)\nprint(nodes)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWBtDBOPRS7C","outputId":"9e202ab1-8b65-4b60-b335-75f8d042083e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create feature extractor","metadata":{"id":"hSKj_3yL000x"}},{"cell_type":"code","source":"from torchvision.models.feature_extraction import create_feature_extractor\n\nfeature_extractor_test = create_feature_extractor(\n\talexnet, return_nodes=['features.10', 'features.11', 'features.12', 'avgpool', 'flatten', 'classifier.4'])\n# `out` will be a dict of Tensors, each representing a feature map\nout = feature_extractor_test(torch.zeros_like(input_batch))","metadata":{"id":"PizXMt9-XXc2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizing the feature vector","metadata":{"id":"ub8aYRux08NV"}},{"cell_type":"code","source":"print(out['classifier.4'].size())\nfeat = out['classifier.4'].detach().numpy().squeeze()\nplt.bar(range(len(feat)), feat)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"id":"Py4whEc1Xy4G","outputId":"d1cbb689-0351-4193-d560-082a2b3f8720","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How does it look for the example image?","metadata":{"id":"rUoNr9ga1B6D"}},{"cell_type":"code","source":"out = feature_extractor_test(input_batch)\nfeat = out['classifier.4'].detach().numpy().squeeze()\nplt.bar(range(len(feat)), feat)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"id":"cQxgHzvOem8Y","outputId":"09c73310-129b-4018-b3b4-b817d1702bd4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training a decoder from the brain activity**","metadata":{"id":"Y2tuCeBLPD3Q"}},{"cell_type":"markdown","source":"Setting up the relevant data from the dataset","metadata":{"id":"hz4U7vytgp3E"}},{"cell_type":"code","source":"roi = 'V4'\nidx = dat[\"roi_names\"].tolist().index(roi)\nprint(dat[\"roi\"] == idx)\nresponses = dat['responses'][20]\nx = responses[dat[\"roi\"] == idx]\nprint(x.shape)\n\n# Setting up input size\nnum_classes = len(feat)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_--UBAFRnMRT","outputId":"3277b9e6-9704-4ab2-de5f-72ab50b3681c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CONFIGURE**","metadata":{"id":"5VvjZHOdBcUa"}},{"cell_type":"code","source":"is_use_collab = False\n\nis_use_alexnet = True\nis_use_vgg19 = False\n\nis_mlp_test = False\nis_dnn_test = True\n\nis_concatenated_test = False\n\nis_training = True\n\nmodel = None","metadata":{"id":"MoT0ZANwj5tl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models import vgg19, VGG19_Weights\nfrom torchvision.models.feature_extraction import get_graph_node_names\n\nvgg19 = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', weights=VGG19_Weights.DEFAULT)\n# or any of these variants\n# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)\n# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)\n# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13_bn', pretrained=True)\n# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)\n# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)\n# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)\n\nnodes, _ = get_graph_node_names(vgg19)\nprint(nodes)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qC37kNTx_MZp","outputId":"73ea9954-cf15-4d38-d1cc-58b1253aef64","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a custom dataset","metadata":{"id":"Sz93nJVu1Opt"}},{"cell_type":"code","source":"dataset_size = dat['stimuli'].shape[0]\ntest_dataset_size = dat['stimuli_test'].shape[0]","metadata":{"id":"nbNlV7Mfkd3K","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomVggDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, is_test = False, is_take_all = True, roi_idx = 0):\n\n        if is_test:\n          self.size = test_dataset_size\n        else:\n          self.size = dataset_size\n\n        self.dataset = dataset.copy()\n        self.is_take_all = is_take_all\n        self.roi_idx = roi_idx\n        self.is_test = is_test\n\n    def __getitem__(self, index):\n\n        stimuli = 'stimuli'\n        responses = 'responses'\n\n        if self.is_test:\n          stimuli+='_test'\n          responses+='_test'\n\n        # Select image (stimuli)\n        im = self.dataset[stimuli][index].copy()\n\n        preprocess = transforms.Compose([\n            # transforms.ToPILImage()\n            transforms.Resize((224,224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\n\n        nim = np.stack([im,im,im],axis=2)*255\n        pim = Image.fromarray(np.uint8(nim)) # To PIL\n        input_tensor = preprocess(pim)\n        input_batch = input_tensor.clone().unsqueeze(0)\n\n        # Get vgg19 feature vector\n        eval_feature_extractor = create_feature_extractor(vgg19, return_nodes=['classifier.3']) # change this for experiment\n        out = eval_feature_extractor(input_batch)\n        y = out['classifier.3'].detach().squeeze() # change this for experiment - feature vector is a label\n\n        # Select vector of voxels\n        responses = self.dataset[responses]\n        if self.is_take_all == True:\n          x = torch.Tensor(responses[index])\n        else:\n          x = torch.Tensor(responses[index][self.dataset[\"roi\"] == self.roi_idx])\n\n        return x, y # Data is responses and Label is Feature Vectors extracted from alexnet\n\n    def __len__(self):\n        return self.size\n\n    def copy(self):\n      return self","metadata":{"id":"z0C4sejG_0-O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomAlexnetDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, is_test = False, is_take_all = True, roi_idx = 0):\n\n        if is_test:\n          self.size = test_dataset_size\n        else:\n          self.size = dataset_size\n\n        self.dataset = dataset.copy()\n        self.is_take_all = is_take_all\n        self.roi_idx = roi_idx\n        self.is_test = is_test\n\n    def __getitem__(self, index):\n        stimuli = 'stimuli'\n        responses = 'responses'\n\n        if self.is_test:\n          stimuli+='_test'\n          responses+='_test'\n\n        # Select image (stimuli)\n        im = self.dataset[stimuli][index]\n\n        preprocess = transforms.Compose([\n            # transforms.ToPILImage()\n            transforms.Resize((224,224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\n\n        nim = np.stack([im,im,im],axis=2)*255\n        pim = Image.fromarray(np.uint8(nim)) # To PIL\n        input_tensor = preprocess(pim)\n        input_batch = input_tensor.clone().unsqueeze(0)\n\n        # Get alexnet feature vector\n        feature_extractor = create_feature_extractor(alexnet, return_nodes=['classifier.4']) # change this for experiment\n        out = feature_extractor(input_batch)\n        y = out['classifier.4'].detach().squeeze() # change this for experiment - feature vector is a label\n\n        # Select vector of voxels\n        responses = self.dataset[responses]\n        if self.is_take_all == True:\n          x = torch.Tensor(responses[index])\n        else:\n          x = torch.Tensor(responses[index][self.dataset[\"roi\"] == self.roi_idx])\n\n        return x, y # Data is responses and Label is Feature Vectors extracted from alexnet\n\n    def __len__(self):\n        return self.size\n\n    def copy(self):\n      return self\n","metadata":{"id":"mJPZYJcxWrcr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomConcatDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, is_test = False, is_take_all = True, roi_idx = 0):\n\n        if is_test:\n          self.size = test_dataset_size\n        else:\n          self.size = dataset_size\n\n        self.dataset = dataset.copy()\n        self.is_take_all = is_take_all\n        self.roi_idx = roi_idx\n        self.is_test = is_test\n\n    def __getitem__(self, index):\n\n        stimuli = 'stimuli'\n        responses = 'responses'\n\n        if self.is_test:\n          stimuli+='_test'\n          responses+='_test'\n\n        # Select image (stimuli)\n        im = self.dataset[stimuli][index].copy()\n\n        preprocess = transforms.Compose([\n            # transforms.ToPILImage()\n            transforms.Resize((224,224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\n\n        nim = np.stack([im,im,im],axis=2)*255\n        pim = Image.fromarray(np.uint8(nim)) # To PIL\n        input_tensor = preprocess(pim)\n        input_batch = input_tensor.clone().unsqueeze(0)\n\n        # Get vgg19 feature vector\n        eval_vgg19_feature_extractor = create_feature_extractor(vgg19, return_nodes=['classifier.3']) # change this for experiment\n        out_vgg = eval_vgg19_feature_extractor(input_batch).copy()\n        y_vgg19 = out_vgg['classifier.3'].detach().squeeze()\n\n        # Get alexnet feature vector\n        alexnet_feature_extractor = create_feature_extractor(alexnet, return_nodes=['classifier.4']) # change this for experiment\n        out_alexnet = alexnet_feature_extractor(input_batch).copy()\n        y_alexnet = out_alexnet['classifier.4'].detach().squeeze() # change this for experiment - feature vector is a label\n\n        y = np.concatenate((y_alexnet, y_vgg19), axis=0)\n\n        # Select vector of voxels\n        responses = self.dataset[responses]\n        if self.is_take_all == True:\n          x = torch.Tensor(responses[index])\n        else:\n          x = torch.Tensor(responses[index][self.dataset[\"roi\"] == self.roi_idx])\n\n        return x, y # Data is responses and Label is Feature Vectors extracted from alexnet\n\n    def __len__(self):\n        return self.size\n\n    def copy(self):\n      return self","metadata":{"id":"bkXXoklJU-y4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's train our decoder. Is it doing a good job? (Don't forget to activate the GPU)\n\n*Exercise:* Compute the accuracy for each empoch","metadata":{"id":"nnXwzqhr1c_o"}},{"cell_type":"code","source":"import torch.nn as nn\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass MLP_Decoder(nn.Module):\n    def __init__(self, hidden_size = 4096, num_classes = 4096, input_size = 8428):\n        super(MLP_Decoder, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\n","metadata":{"id":"iaVseTd8d36B","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass DNN_Decoder(nn.Module):\n    def __init__(self, hidden_size, num_classes, input_size = 8428):\n        super(DNN_Decoder, self).__init__()\n        self.hidden_size = int(input_size/2)\n        \n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu1 = nn.ReLU()\n        \n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.relu2 = nn.ReLU()\n        \n        self.fc3 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        return out\n","metadata":{"id":"mCdbrCmfnYqN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchviz import make_dot\n\n# Define a simple PyTorch model\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.fc1 = nn.Linear(10, 5)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(5, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# Instantiate the model\nmodel = SimpleModel()\n\n# Create a dummy input (batch size of 1, input size of 10)\ndummy_input = torch.randn(1, 10)\n\n# Generate a dynamic computation graph\noutput = model(dummy_input)\ngraph = make_dot(output, params=dict(model.named_parameters()))\n\n# Save the graph as a PNG file\ngraph.render(\"simple_model\", format=\"png\", cleanup=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T14:31:05.881266Z","iopub.execute_input":"2024-01-04T14:31:05.882124Z","iopub.status.idle":"2024-01-04T14:31:05.925183Z","shell.execute_reply.started":"2024-01-04T14:31:05.882087Z","shell.execute_reply":"2024-01-04T14:31:05.924041Z"},"trusted":true},"execution_count":474,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[474], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define a simple PyTorch model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"],"ename":"ModuleNotFoundError","evalue":"No module named 'torchviz'","output_type":"error"}]},{"cell_type":"markdown","source":"PREPARE THE DATASET","metadata":{"id":"u-PKBqcAlC00"}},{"cell_type":"code","source":"custom_dataset = None\ncustom_test_dataset = None\n\n\n# You can then use the prebuilt data loader.\nif is_use_alexnet:\n  custom_dataset = CustomAlexnetDataset(dat)\n  custom_test_dataset = CustomAlexnetDataset(dat, is_test = True)\nelif is_use_vgg19:\n  custom_dataset = CustomVggDataset(dat)\n  custom_test_dataset = CustomVggDataset(dat, is_test = True)\nelif is_concatenated_test:\n    custom_dataset = CustomConcatDataset(dat)\n    custom_test_dataset = CustomConcatDataset(dat, is_test = True)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=custom_dataset.copy(),\n                                           batch_size=64,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=custom_test_dataset.copy(),\n                                           batch_size=64,\n                                           shuffle=True)","metadata":{"id":"PZlO9PIzlBTw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's train our decoder. Is it doing a good job? (Don't forget to activate the GPU)\n\n*Exercise:* Compute the accuracy for each empoch","metadata":{"id":"K8q6L5kCd36D"}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nif is_training:\n  num_epochs = 30\n\n  if is_use_vgg19:\n    num_epochs = 20\n  # Train the decoder\n  learning_rate = 0.001\n  hidden_size = 4096\n\n  if is_concatenated_test:\n    hidden_size = hidden_size*2\n    num_classes = num_classes*2\n\n  if is_mlp_test:\n    model = MLP_Decoder(hidden_size, num_classes).to(device)\n  elif is_dnn_test:\n    model = DNN_Decoder(hidden_size, num_classes, input_size = 8428).to(device)\n\n  # define loss function - optimization - lr scheduler\n  criterion = nn.MSELoss()\n  optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.01, lr=learning_rate)\n  # using ReduceLROnPlateau to reduce lr when there's no improvement with loss or accuracy\n  scheduler = ReduceLROnPlateau(optimizer, patience = 3, factor = 0.1, min_lr=0.000001)\n\n  total_step = len(train_loader)\n\n  for epoch in range(num_epochs):\n      total_loss = 0.0\n      print(\"Running epoch {} with learning rate {}\".format(epoch+1, optimizer.param_groups[0]['lr']))\n      for i, (voxels, afeatures) in enumerate(train_loader):\n          optimizer.zero_grad()\n          # Move tensors to the configured device\n          voxels = voxels.to(device)\n          afeatures = afeatures.to(device)\n\n          # Forward pass\n          outputs = model(voxels)\n          loss = criterion(outputs, afeatures)\n\n          # Backward and optimize\n          loss.backward()\n          optimizer.step()\n          total_loss += loss.item()\n\n          if (i+1) % 10 == 0 or (i+1) % total_step == 0:\n              print ('Step [{}/{}], Loss: {:.4f}'\n                    .format(i+1, total_step, loss.item()))\n      average_loss = total_loss / len(train_loader)\n      validation_loss = average_loss\n      scheduler.step(validation_loss)\n\n      print(f'Epoch [{epoch + 1}/{num_epochs}], Current Loss: {average_loss:.4f}, Learning Rate: {optimizer.param_groups[0][\"lr\"]:.4f}')\n","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"obAf1foPd36D","outputId":"84c1dea0-c441-4018-ae16-b7f035ea9ec3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Example integer arrays\narray1 = np.array([1, 2, 3])\narray2 = np.array([4, 5, 6])\n\n# Concatenate arrays along a specified axis\nresult = np.concatenate((array1, array2), axis=0)\n\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pickle\n\nassessment_path = ''\n\nif is_use_collab:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    assessment_path = '/content/drive/MyDrive/DEEP NEURAL NETWORKS AND LEARNING SYSTEMS/ASSESSMENT/'\n\nmodel_path = f'{assessment_path}'\n\nif is_training:\n    if is_use_alexnet:\n        model_path+='alexnet'\n    elif is_use_vgg19:\n        model_path+='vgg19'\n    elif is_concatenated_test:\n        model_path+='concatenated'\n    \n    if is_mlp_test:\n        model_path+='_mlp_decoder.pkl'\n    elif is_dnn_test:\n        model_path+='_dnn_decoder.pkl'\n        \n    with open(model_path,'wb') as handle:\n        pickle.dump(model, handle)\n\n  #cosine similarities","metadata":{"id":"NtGQak95sS6m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_with_model(test_dl, model):\n  y_preds, y_tests = list(), list()\n  model.eval()\n  test_loss = 0\n  with torch.no_grad():\n    for i, (input, targets) in enumerate(test_dl):\n      # Evaluate the model on the test set\n      input = input.to(device)\n      targets = targets.to(device)\n      y_hat = model(input)\n      test_loss = criterion(y_hat, targets).item()\n      # Retrieve numpy array\n      y_hat = np.array(y_hat.detach().cpu())\n      y_test = np.array(targets.cpu())\n      # y_test = y_test.reshape((len(y_test), 1))\n      print('y_hat: ', y_hat.shape)\n      print('y_test: ', y_test.shape)\n      # Round to class values\n      y_hat = y_hat.round()\n\n      # Store\n      y_preds.append(y_hat)\n      y_tests.append(y_test)\n    test_loss /= len(test_dl)\n    y_preds, y_tests = np.vstack(y_preds), np.vstack(y_tests)\n    print('\\nTest set: Average loss: {:.4f}'.format(test_loss))\n    return y_preds, y_tests, test_loss","metadata":{"id":"Qd5uTybW5nfu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nassessment_path = ''\n\nif is_use_collab:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    import pickle\n    assessment_path = '/content/drive/MyDrive/DEEP NEURAL NETWORKS AND LEARNING SYSTEMS/ASSESSMENT/'\n\nmodel_path = f'{assessment_path}'\n\nif is_use_alexnet:\n  model_path+='alexnet'\nelif is_use_vgg19:\n  model_path+='vgg19'\nelif is_concatenated_test:\n    model_path+='concatenated'\n\nif is_mlp_test:\n  model_path+='_mlp_decoder.pkl'\nelif is_dnn_test:\n  model_path+='_dnn_decoder.pkl'\nprint(model_path)\nwith open(model_path,'rb') as file:\n  load_model = pickle.load(file)\n\npredictions, actuals, test_loss = predict_with_model(test_loader, load_model)\n\n# use othe","metadata":{"id":"eb9YCGthN-4B","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(predictions.shape)\nprint(actuals.shape)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nprint(\"pred_feat: \", predictions.shape)\nprint(\"pretrained_feat: \", actuals.shape)\n\ncosine_preds = predictions.copy()\ncosine_actuals = actuals.copy()\n\nsimilarity = []\n\nfor i in range(predictions.shape[0]):\n  predictions[i] = predictions[i]/np.linalg.norm(predictions[i])\n  actuals[i] = actuals[i]/np.linalg.norm(actuals[i])\ndot_product = np.dot(predictions, actuals.T)\n\n\ncolors = [(0, 0, 0), (0, 0, 1)]  # (black, red)\nn_bins = 100  # Number of bins\ncmap_name = \"black_blue\"\ncustom_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n# Plot the matrix as a heatmap\nplt.imshow(dot_product, cmap=custom_cmap, interpolation='nearest')\n\nplt.xlabel('Predicted feature vectors')\nplt.ylabel('Actual feature vectors')\n# Add a colorbar to the right of the plot\nplt.colorbar()\n\n# Show the plot\nplt.show()","metadata":{"id":"460r_0IU5spj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's train our decoder. Is it doing a good job? (Don't forget to activate the GPU)\n\n*Exercise:* Compute the accuracy for each empoch","metadata":{"id":"oxmkiYNymMGT"}},{"cell_type":"code","source":"# First select an image from the validation\n\n# Compute the features\n\n# Compute the feature vector from alexnet for the same image\n\n# COmpute the dot product\n\n# Create matrix comparing at least 20 images","metadata":{"id":"VtQpFm7cH3NB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Average feature vector for each category**\nOn the validation set, take the features extracted from alexnet and compute an average feature set for each category","metadata":{"id":"oirDr8s8w1Qp"}},{"cell_type":"code","source":"# Your code here","metadata":{"id":"urI628iKxJrC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decode activity**\nTake the ROI voxel activations for each image in the validation set and decode the corresponding feature vector","metadata":{"id":"rz370Br5xLo2"}},{"cell_type":"code","source":"# your code here\n# predict responses to get feature vector","metadata":{"id":"StfjbMlaxehq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compare the averae with the decoded features** Iterate over all averages and compare the two vectors, average and decode. The best match is the corresponding category.","metadata":{"id":"1iE9y58Cxgzh"}},{"cell_type":"code","source":"# Your code here\n# get result from 2 cells above and compare with dot product","metadata":{"id":"ffYj9eQMx4Rh","trusted":true},"execution_count":null,"outputs":[]}]}